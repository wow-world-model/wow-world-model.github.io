<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>WoW</title>
  <link rel="icon" href="./figs/logo.png" type="image/png">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="toc">
    <h3>Content</h3>
    <hr>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#real-world-scenarios">Real-world Scenarios</a></li>
      <li><a href="#robot-manipulation">Robot Manipulation</a></li>
      <li><a href="#physics-simulation">Physics Simulation</a></li>
      <li><a href="#counterfact-imagination">Counterfact Imagination</a></li>
      <li><a href="#long-horizon-tasks"></a>Long Horizon Tasks</li>
      <li><a href="#artistic-interaction">Artistic Interaction</a></li>
      <li><a href="#approach">Experimental Results</a></li>
    </ul>
  </div>

  <div class="main-content">
    <div class="hero-text" style="font-size: 100px; text-align: center; margin-bottom: -10px;">WoW</div>
    <div class="sub-hero-text" style="text-align: center;">
      Towards a World-omniscient World-model Through Embodied Interaction
    </div>

    <!-- Authors -->
<div class="authors" style="text-align: center; color: #000000;">
  <span class="author-block" style="font-weight: bold;">
    Xiaowei Chi<sup>1,2,3</sup><sup>&dagger;</sup>, 
    Peidong Jia<sup>1,2</sup><sup>&dagger;</sup>, 
    Chun-Kai Fan<sup>1,2</sup><sup>&dagger;</sup>, 
    Xiaozhu Ju<sup>1</sup><sup>&dagger;</sup>, 
    Weishi Mi<sup>1</sup><sup>&dagger;</sup>, 
    Kevin Zhang<sup>2</sup>, 
    Zhiyuan Qin<sup>1</sup>, 
    Wanxin Tian<sup>1</sup>, 
    Kuangzhi Ge<sup>2</sup>, 
    Hao Li<sup>1</sup>, 
    Zezhong Qian<sup>1,2</sup>, 
    Anthony Chen<sup>2</sup>, 
    Qiang Zhou<sup>1,2</sup>,  
    Yueru Jia<sup>2</sup>, 
    Jiaming Liu<sup>2</sup>, 
    Yong Dai<sup>1</sup>, 
    Qingpo Wuwu<sup>2</sup>, 
    Chengyu Bai<sup>2</sup>, 
    Yu-Kai Wang<sup>2</sup>, 
    Ying Li<sup>2</sup>,  
    Lizhang Chen<sup>1,2</sup>, 
    Yong Bao<sup>1</sup>, 
    Zhiyuan Jiang<sup>1</sup>, 
    Jiacheng Zhu<sup>1</sup>, 
    Kai Tang<sup>2</sup>, 
    Ruichuan An<sup>2</sup>, 
    Yulin Luo<sup>2</sup>, 
    Qiuxuan Feng<sup>1,2</sup>, 
    Siyuan Zhou<sup>3</sup>, 
    Chi-min Chan<sup>3</sup>, 
    Chengkai Hou<sup>1,2</sup>, 
    Wei Xue<sup>3</sup>, 
    Sirui Han<sup>3</sup>, 
    Yike Guo<sup>3</sup>, 
    <span style="font-family: 'Helvetica', Arial, sans-serif; font-size: 16px;">
      <a href="https://www.shanghangzhang.com" style="text-decoration: none; color: #252525;">Shanghang Zhang</a><sup>2,</sup><sup>&#x2709;</sup>
      <a href="mailto:shanghang" title="Contact Shanghang Zhang via email">
        <i class="fas fa-envelope" style="color: #e91e63; font-size: 0.8em; margin-left: 4px;"></i>
      </a>
      &nbsp;&nbsp;
      <a href="https://jian-tang.com" style="text-decoration: none; color: #252525;">Jian Tang</a><sup>1,</sup><sup>&#x2709;</sup>
      <a href="mailto:jiantang" title="Contact Jian Tang via email">
        <i class="fas fa-envelope" style="color: #e91e63; font-size: 0.8em; margin-left: 4px;"></i>
      </a>
    </span>
  </span>
  <br>
  <span class="affiliation" style="font-weight: normal;">
    <sup>1</sup> Beijing Innovation Center of Humanoid Robotics<br>
    <sup>2</sup> State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University<br>
    <sup>3</sup> Hong Kong University of Science and Technology
  </span>
</div>

    <div class="links-block">
      <a href="https://arxiv.org/pdf/2509.22642" target="_blank">[<strong>pdf</strong>]</a>
      <a href="https://arxiv.org/abs/2509.22642" target="_blank">[<strong>arxiv</strong>]</a>
      <a href="https://github.com/wow-world-model/wow-world-model" target="_blank">[<strong>code</strong>]</a>
    </div>

    <div class="tagline" id="abstract">Abstract.</div>

    <img src="./figs/teaser.png"></img>
    <p class="figure-caption">
        <b>WoW world model</b> generates high-quality, physically consistent robot action videos in Out-of-Distribution (OOD) scenarios, enabling closed-loop corrections and real-world robotic execution. The illustration shows the model's strong generalization across diverse tasks and environments.
    </p>

    <div class="section">
      <p style="text-align: justify;">
      Humans develop an understanding of intuitive physics through active interaction with the world. In stark contrast, current video models such as Sora rely solely on passive observation and therefore struggle with grasping physical causality. This motivates our central hypothesis: <em>authentic physical intuition in a world model must be grounded in extensive, causally rich interactions with the real world</em>. To test this, we introduce <strong>WoW</strong>, a 14B-parameter generative world model trained on 2 million real-world robot interaction trajectories. We find that the modelâ€™s understanding of physics emerges as a probabilistic distribution of plausible outcomes, which can lead to stochastic instabilities and physical hallucinations. To mitigate these, we propose <em>SOPHIA</em>, a novel vision-language agent that evaluates the output of the DiT model and iteratively refines the language instructions to steer generation toward physical realism. Complementing this, a co-trained <em>Inverse Dynamics Model</em> translates refined plans into executable robotic actions, effectively closing the imagination-to-action loop. We further establish <strong>WoWBench</strong>, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluations, excelling in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is essential for developing physical intuition in AI. 
      </p>
    </div>
    
    
    
    <!-- Kitchen and Conditional Reasoning Videos -->
    <div class="tagline" id="real-world-scenarios">Real-world Scenarios.</div>
    <div class="video-gallery-section" id="scenariosGallerySection">
      <div class="video-gallery-container">
        <div class="video-gallery" id="videoGalleryScenarios">
          <video class="gallery-video" src="./static/wow-videos/robots/jobs2.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/light.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/life/tube.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/life/mask5.mp4" autoplay muted playsinline loop></video>
          <!-- <video class="gallery-video" src="./static/wow-videos/robots/stack_cups2_long-term.mp4" autoplay muted playsinline loop></video> -->
          <!-- <video class="gallery-video" src="./static/wow-videos/robots/00000244-manipulate_clothes.mp4" autoplay muted playsinline loop></video> -->
        </div>
      </div>
      <div class="gallery-caption-container">
        <div class="gallery-nav-controls">
          <button class="gallery-nav left" id="scrollLeftBtnScenarios">&lt;</button>
          <button class="gallery-nav right" id="scrollRightBtnScenarios">&gt;</button>
        </div>
        <p class="figure-caption gallery-caption">
          <b>Zero-shot daily Scenarios:</b> Natural environments and daily life activities including workplace tasks, and everyday interactions demonstrating practical applicability.
        </p>
      </div>
    </div>
    
    
    <!-- Robot Manipulation Videos Gallery -->
    <div class="tagline" id="robot-manipulation">Robot Manipulation.</div>
    <div class="video-gallery-section" id="robotsGallerySection">
      <div class="video-gallery-container">
        <div class="video-gallery" id="videoGalleryRobots">
              <video class="gallery-video" src="./static/wow-videos/robots/kitchen2.mp4" autoplay muted playsinline loop></video>
              <!-- <video class="gallery-video" src="./static/wow-videos/robots/00000056-manipulate_rope.mp4" autoplay muted playsinline loop></video> -->
              <video class="gallery-video" src="./static/wow-videos/robots/kitchen3.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/robots/unitree.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/robots/tiankunhello2.mp4" autoplay muted playsinline loop></video>
              <!-- <video class="gallery-video" src="./static/wow-videos/robots/00000225-manipulate_rope.mp4" autoplay muted playsinline loop></video> -->
            </div>
          </div>
          <div class="gallery-caption-container">
            <div class="gallery-nav-controls">
              <button class="gallery-nav left" id="scrollLeftBtnRobots">&lt;</button>
              <button class="gallery-nav right" id="scrollRightBtnRobots">&gt;</button>
            </div>
            <p class="figure-caption gallery-caption">
              <b>Zero-shot Generation of front view Humanoid robots</b> in diverse environments, a novel front view, showcasing the model's ability to understand and predict complex interactions in dynamic settings.
            </p>
          </div>
        </div>
      
    <div class="tagline" id="physics-simulation">Physics Simulation.</div>
    
    <!-- Physics Videos Gallery -->
    <div class="video-gallery-section" id="physicsGallerySection">
      <div class="video-gallery-container">
        <div class="video-gallery" id="videoGalleryPhysics">
          <!-- <video class="gallery-video" src="./static/wow-videos/physics/light.mp4" autoplay muted playsinline loop></video> -->
          <video class="gallery-video" src="./static/wow-videos/physics/counterfact_task1_part1.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/crush_rabbit.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/sticky_cube.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/three_cubes_fire_soft_water.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/counterfact_task1_part3.mp4" autoplay muted playsinline loop></video>
          <!-- <video class="gallery-video" src="./static/wow-videos/physics/cube_splitting.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/cube_flower.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./static/wow-videos/physics/cube_splitting2.mp4" autoplay muted playsinline loop></video> -->
        </div>
      </div>
      <div class="gallery-caption-container">
        <div class="gallery-nav-controls">
          <button class="gallery-nav left" id="scrollLeftBtnPhysics">&lt;</button>
          <button class="gallery-nav right" id="scrollRightBtnPhysics">&gt;</button>
        </div>
        <p class="figure-caption gallery-caption">
          <b>Physics Simulation:</b> WoW demonstrates sophisticated understanding of physical properties including object splitting, deformation, lighting effects, adhesion, and counterfactual reasoning.
        </p>
      </div>
    </div>

        
        <!-- Cubes Showcase Grid -->
        <div class="tagline" id="counterfact-imagination">counterfact Imagination</div>
        <div class="video-grid-container">
          <div class="video-grid">
            <video class="grid-video" src="./static/wow-videos/cubes-future/45-degree.mp4" autoplay muted playsinline loop></video>
            <video class="grid-video" src="./static/wow-videos/cubes-future/dumn.mp4" autoplay muted playsinline loop></video>
            <video class="grid-video" src="./static/wow-videos/cubes-future/magnetic.mp4" autoplay muted playsinline loop></video>
            <video class="grid-video" src="./static/wow-videos/cubes-future/water.mp4" autoplay muted playsinline loop></video>
            <video class="grid-video" src="./static/wow-videos/cubes-future/alive.mp4" autoplay muted playsinline loop></video>
            <video class="grid-video" src="./static/wow-videos/cubes-future/acidlemon.mp4" autoplay muted playsinline loop></video>
            
          </div>
        </div>
        <p class="figure-caption">
          <b>counterfact Imagination:</b> Diverse counterfactual scenarios showcasing WoW's ability to imagine and generate physically plausible outcomes under altered conditions, such as different angles, magnetic properties, and energy effects.
        </p>
        
        <div class="tagline" id="long-horizon-tasks">Long Horizon Tasks.</div>
        
        <!-- Long Horizon Videos -->
        <div class="video-gallery-section" id="artGallerySection">
          <div class="video-gallery-container">
            <div class="video-gallery" id="videoGalleryArt">
              <video class="gallery-video" src="./static/wow-videos/robots/stack_cups2_long-term.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/robots/stack_cups_long-term.mp4" autoplay muted playsinline loop></video>
              <!-- <video class="gallery-video" src="./static/wow-videos/life/pour2.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/life/wujing.mp4" autoplay muted playsinline loop></video> -->
            </div>
          </div>
          <div class="gallery-caption-container">
            <div class="gallery-nav-controls">
              <button class="gallery-nav left" id="scrollLeftBtnArt">&lt;</button>
              <button class="gallery-nav right" id="scrollRightBtnArt">&gt;</button>
            </div>
            <p class="figure-caption gallery-caption">
              <b>Long Horizon Tasks.</b> WoW excels in generating extended sequences of robot actions, demonstrating its capability to plan and execute complex tasks over longer time horizons with sustained physical consistency.
            </p>
          </div>
        </div>
        


        <!-- Artistic Videos -->
        <div class="tagline" id="artistic-interaction">Artistic Interaction.</div>
        <div class="video-gallery-section" id="artGallerySection">
          <div class="video-gallery-container">
            <div class="video-gallery" id="videoGalleryArt">
              <video class="gallery-video" src="./static/wow-videos/art/Sunflower.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/art/van_Gogh.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/life/pour2.mp4" autoplay muted playsinline loop></video>
              <video class="gallery-video" src="./static/wow-videos/life/wujing.mp4" autoplay muted playsinline loop></video>
            </div>
          </div>
          <div class="gallery-caption-container">
            <div class="gallery-nav-controls">
              <button class="gallery-nav left" id="scrollLeftBtnArt">&lt;</button>
              <button class="gallery-nav right" id="scrollRightBtnArt">&gt;</button>
            </div>
            <p class="figure-caption gallery-caption">
              <b>Artistic Interaction:</b> WoW demonstrates understanding of 2D-3D relationships by enabling robotic hands to extract objects from famous paintings, showcasing cross-modal reasoning between visual art and physical manipulation.
            </p>
          </div>
        </div>
        <div class="tagline" id="video-to-video-transfer">More Result & Open Source Plan.</div>
        <div style="display: flex; flex-direction: column; align-items: flex-start; margin: 32px 0;">
          <label style="display: flex; align-items: center; font-size: 1.2em;">
            <span>
              <input type="checkbox" style="margin-right: 8px;">
              <b>DiT/UNet checkpoints:</b> We will update the SVD, CogVideoX, Cosmos1&2, Wan-14B, and especially, our WoW-DiT 2B,7B, 14B, model group in Oct. <br>
              <input type="checkbox" style="margin-right: 8px;">
              <b>Video-to-Video Transfer:</b> Stay tuned for updates! <br>
              <input type="checkbox" style="margin-right: 8px;">
              <b>Real-world VLA demo coming soon:</b> Stay tuned for updates!<br>
              <input type="checkbox" style="margin-right: 8px;">
              <b>Online testing demo: </b> Stay tuned for updates!<br>
            </span>
          </label>
        </div>
        
        <!-- <div class="tagline" id="approach">Approach.</div>
        
        <div class="r2s-vertical-layout">
          <div class="r2s-video-row r2s-top-row">
            <img class="r2s-video-item r2s-video-input" src="./static/images/WoW/method1.png" autoplay muted loop playsinline></img>
          </div>
          <div class="r2s-caption-row">
            <p class="r2s-caption-item">Model Overview</p>
          </div>
          <p class="figure-caption">
            <b>(a) Inference: a latent diffusion transformer predicts future frames from image observations and text-based action descriptions. (b) Training: DINO features supervise intermediate DiT representations via a token relation distillation loss to improve spatial-temporal modeling.</b>
          </p>
        </div> -->
<!--         
        <div class="tagline" id="approach">Experimental Results.</div>
        
        <div class="r2s-vertical-layout">
          <div class="r2s-video-row r2s-top-row">
            <img class="r2s-video-item r2s-video-input" src="./static/images/1_Table.png"></img>
          </div>
          <div class="r2s-caption-row">
            <p class="r2s-caption-item"><b>Scaling Law Comparison at Varying Data Scales.</b> Performance comparison across four different dataset sizes (30k, 200k, 600k, 2M samples) on multiple benchmarks, demonstrating clear improvements with increased training data following established neural scaling laws.</p>
          </div>
        </div>
        
        <div class="r2s-vertical-layout">
          <div class="r2s-video-row r2s-top-row">
            <img class="r2s-video-item r2s-video-input" src="./static/images/2_Table.png" style="height: auto; max-height: none;"></img>
          </div>
          <div class="r2s-caption-row">
            <p class="r2s-caption-item"><b>Comparative Analysis Across Video Generation Models.</b> Comprehensive evaluation of different models including CogVideoX, Cosmos-Predict, Wan2.1, and our proposed WoW across multiple metrics: Video Quality (VQ), Instruction Understanding (IU), Physical Law (PL), and Planning. Results show both human evaluation and autonomous evaluation scores, with WoW achieving superior performance across most dimensions.</p>
          </div>
        </div>
         -->
        <style>
  .r2s-video-item {
    width: 100%;
    height: auto !important;
    max-height: none !important;
    object-fit: contain !important;
  }
  </style>

<div class="bibtex-code">
  <div class="bibtex-title">BibTeX</div>
  <pre><code>@misc{chi2025wowworldomniscientworld,
      title={WoW: Towards a World omniscient World model Through Embodied Interaction}, 
      author={Xiaowei Chi and Peidong Jia and Chun-Kai Fan and Xiaozhu Ju and Weishi Mi and Kevin Zhang and Zhiyuan Qin and Wanxin Tian and Kuangzhi Ge and Hao Li and Zezhong Qian and Anthony Chen and Qiang Zhou and Yueru Jia and Jiaming Liu and Yong Dai and Qingpo Wuwu and Chengyu Bai and Yu-Kai Wang and Ying Li and Lizhang Chen and Yong Bao and Zhiyuan Jiang and Jiacheng Zhu and Kai Tang and Ruichuan An and Yulin Luo and Qiuxuan Feng and Siyuan Zhou and Chi-min Chan and Chengkai Hou and Wei Xue and Sirui Han and Yike Guo and Shanghang Zhang and Jian Tang},
      year={2025},
      eprint={2509.22642},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2509.22642}, 
}</code></pre>
    </div>
  </div>
<div style="width: 20%; position:relative; left:40%">
  <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=o82dCBYQYuFjyki78TuW83nhdP5vAceJk-N08qEcGgM&cl=ffffff&w=a"></script>
</div>

  <!-- JavaScript for Video Gallery Navigation -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const galleries = [
        {
          sectionId: 'physicsGallerySection',
          galleryInnerId: 'videoGalleryPhysics',
          scrollLeftBtnId: 'scrollLeftBtnPhysics',
          scrollRightBtnId: 'scrollRightBtnPhysics'
        },
        {
          sectionId: 'robotsGallerySection',
          galleryInnerId: 'videoGalleryRobots',
          scrollLeftBtnId: 'scrollLeftBtnRobots',
          scrollRightBtnId: 'scrollRightBtnRobots'
        },
        {
          sectionId: 'scenariosGallerySection',
          galleryInnerId: 'videoGalleryScenarios',
          scrollLeftBtnId: 'scrollLeftBtnScenarios',
          scrollRightBtnId: 'scrollRightBtnScenarios'
        },
        {
          sectionId: 'artGallerySection',
          galleryInnerId: 'videoGalleryArt',
          scrollLeftBtnId: 'scrollLeftBtnArt',
          scrollRightBtnId: 'scrollRightBtnArt'
        }
      ];

      galleries.forEach(galleryConfig => {
        const gallerySection = document.getElementById(galleryConfig.sectionId);
        if (!gallerySection) return;

        const galleryContainer = gallerySection.querySelector('.video-gallery-container');
        const galleryInner = document.getElementById(galleryConfig.galleryInnerId);
        const scrollLeftBtn = document.getElementById(galleryConfig.scrollLeftBtnId);
        const scrollRightBtn = document.getElementById(galleryConfig.scrollRightBtnId);

        if (galleryContainer && galleryInner && scrollLeftBtn && scrollRightBtn) {
          const scrollAmount = (galleryInner.firstElementChild?.offsetWidth || 300) + 15;

          scrollLeftBtn.addEventListener('click', () => {
            galleryContainer.scrollBy({ left: -scrollAmount, behavior: 'smooth' });
          });

          scrollRightBtn.addEventListener('click', () => {
            galleryContainer.scrollBy({ left: scrollAmount, behavior: 'smooth' });
          });
        }
      });
    });
  </script>

  <style>
    .video-grid-container {
      margin: 20px 0;
    }
    
    .video-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 15px;
      margin: 20px 0;
    }
    
    .grid-video {
      width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    @media (max-width: 768px) {
      .video-grid {
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 10px;
      }
    }
  </style>

</body>
</html>
