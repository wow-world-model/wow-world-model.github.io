# Wow: Harnessing Intuitive Physics from a Scalable Embodied World Model

## ðŸ“– Paper

**Wow: Harnessing Intuitive Physics from a Scalable Embodied World Model**  
Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Qiang Zhou, Anthony Chen, Yong Dai, Jiaming Liu, Ying Li, Qingpo Wuwu, Yong Bao, Qiuxuan Feng, Kai Tang *et al.*  

---

## ðŸ”‘ Abstract

Humans build intuitive physics through active interaction with the world, a stark contrast to current video models like Sora, which rely on passive observation and thus fundamentally struggle with physical causality.  

To test this, we introduce **Wow**, a video diffusion model scaled up to **14B parameters** and trained on a dataset of **2 million real-world robot interaction trajectories**. Our central finding is that an implicit understanding of physicsâ€”such as collision and occlusionâ€”directly emerges with scale.  

We further propose **Reflective Refinement**, a loop where a VLM critiques its own predictions and regenerates physically plausible outcomes. For evaluation, we establish **WowBench**, where Wow achieves **86%**, showing strong ability on object recognition, spatial understanding, and material simulation.  
