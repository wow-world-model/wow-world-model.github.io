# Wow: Harnessing Intuitive Physics from a Scalable Embodied World Model

> **This is the project page for [Wow: Harnessing Intuitive Physics from a Scalable Embodied World Model](https://wow-world-model.github.io/)**

## ðŸ“– Paper

**Wow: Harnessing Intuitive Physics from a Scalable Embodied World Model**  
Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Zhiyuan Qin, Kevin Zhang, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Qiang Zhou, Anthony Chen, Yong Dai, Jiaming Liu, Ying Li, Qingpo Wuwu, Yong Bao, Qiuxuan Feng, Kai Tang *et al.*  

![Wow Teaser](./figs/teaser.png)

---

## ðŸ”‘ Abstract

<p style="text-align: justify; text-justify: inter-word;">
World models have recently emerged as a powerful paradigm for robotics, integrating perception, action, and reasoning. However, most existing world models focus on narrow domains or rely heavily on simulation, limiting their ability to generalize to real-world embodied interaction. We introduce <strong>Wow</strong> (Harnessing Intuitive Physics from a Scalable Embodied World Model), a large-scale embodied world model designed to capture intuitive physics and enable scalable robot learning. Wow unifies multimodal inputs from diverse real and simulated datasets, and leverages embodied interaction to learn dynamics that transfer robustly across environments and robot platforms. Across extensive experiments, Wow demonstrates strong generalization in manipulation and long-horizon planning, enabling closed-loop control and real-world deployment. Our results highlight that embodied interaction and large-scale training are critical to building world-omniscient models for robotics.
</p>
